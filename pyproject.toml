[project]
name = "openvino-genai"
version = "2025.2.0.0"
description = "Library of the most popular Generative AI model pipelines, optimized execution methods, and samples"
requires-python = ">=3.9,<=3.13"
readme = { file = "src/README.md", content-type="text/markdown" }
license = { "text" = "Apache-2.0" }
license-files = [
    "LICENSE",
    "third-party-programs.txt",
    "SECURITY.md",
    "README.md"
]
authors = [
    { name = "OpenVINO Developers", email = "openvino@intel.com" },
]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "License :: OSI Approved :: Apache Software License",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Operating System :: Unix",
    "Operating System :: POSIX :: Linux",
    "Operating System :: Microsoft :: Windows",
    "Operating System :: MacOS",
    "Programming Language :: C++",
    "Programming Language :: C",
    "Programming Language :: Python :: 3 :: Only",
    "Programming Language :: Python :: Implementation :: CPython"
]
dependencies = [
    "openvino_tokenizers~=2025.2.0.0.dev"
]

[project.optional-dependencies]
samples = [
    "optimum-intel[nncf] ~= 1.23.0.dev",
    "numpy<2.0.0; sys_platform == 'darwin' and platform_machine == 'x86_64'",
    "einops==0.8.1",  # For Qwen
    "transformers_stream_generator==0.0.5",  # For Qwen
    "diffusers==0.33.1",  # For image generation pipelines
    "timm==1.0.15",  # For exporting InternVL2
    "torchvision==0.22.0",  # For visual language models
    "torch",
    "transformers>=4.43",  # For Whisper
    "hf_transfer",  # For faster models download, should be used with env var HF_HUB_ENABLE_HF_TRANSFER=1
    "librosa==0.11.0",  # For Whisper
    "pillow==11.2.1"  # Image processing for VLMs
]

[tool.py-build-cmake.module]
directory = "src/python"

[tool.py-build-cmake.sdist]
include = ["CMakeLists.txt", "LICENSE", "third-party-programs.txt", "SECURITY.md", "cmake", "src", "thirdparty"]

[tool.py-build-cmake.cmake]
minimum_version = "3.23"
build_type = "Release"
config = ["Release"]
find_python3 = true
build_args = ["--parallel", "--target", "py_openvino_genai_stub"]
install_args = ["--strip"]
install_components = ["wheel_genai"]
options = {"ENABLE_PYTHON" = "ON", "BUILD_TOKENIZERS" = "OFF", "ENABLE_SAMPLES" = "OFF", "CMAKE_SKIP_INSTALL_RPATH" = "OFF"}

[build-system]
requires = [
    "py-build-cmake==0.4.3",
    "openvino~=2025.2.0.0.dev",
    "pybind11-stubgen==2.5.3",
    "cmake~=3.23.0; platform_system != 'Darwin' or platform_machine == 'x86_64'",
    "cmake~=4.0.0; platform_system == 'Darwin' and platform_machine == 'arm64'",
]
build-backend = "py_build_cmake.build"

[tool.pytest.ini_options]
markers = [
    "nightly",
    "precommit: (deselect with '-m \"precommit\"')",
]

[tool.poetry]
package-mode = false

[[tool.poetry.source]]
name = "openvino-nightly"
url = "https://storage.openvinotoolkit.org/simple/wheels/nightly"
priority = "supplemental"

[[tool.poetry.source]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
priority = "supplemental"

[tool.poetry.dependencies]
openvino_tokenizers = { version = "*", source = "openvino-nightly" }
torchvision = {  version = "*", source = "pytorch-cpu" }
torch = {  version = "*", source = "pytorch-cpu" }

[tool.poetry.group.dev]
optional = true

[tool.poetry.group.dev.dependencies]
optimum-intel = { git = "https://github.com/huggingface/optimum-intel.git", rev = "303bc50fdeb34b427dc1b04be741bf4f11415b2d", extras = ["nncf"], allow-prereleases = true }

[tool.poetry.group.tests]
optional = true

[tool.poetry.group.tests.dependencies]
pytest = ">=7.0.0,<=8.3.5"
pytest-html = ">=3.0.0,<=4.1.1"
diffusers = ">=0.30.0,<=0.33.1"
onnx = ">=1.16.0,<=1.17.0"
hf_transfer = ">=0.1.0,<=0.1.9"
gguf = ">=0.9.1,<=0.16.3"

[tool.poetry.group.test_models]
optional = true

[tool.poetry.group.test_models.dependencies]
# requirements for specific models
# - hf-tiny-model-private/tiny-random-RoFormerForCausalLM
rjieba = ">=0.1.10,<=0.1.13"
# - baichuan-inc/Baichuan2-7B-Chat
bitsandbytes = ">=0.45.0,<=0.45.5"
# - nomic-ai/gpt4all-falcon
# - Qwen/Qwen-7B
# - Qwen/Qwen-7B-Chat
# - mosaicml/mpt-7b
# - internlm/internlm2-7b
einops = ">=0.8.0,<=0.8.1"
# - Qwen/Qwen-7B
# - Qwen/Qwen-7B-Chat
transformers_stream_generator = ">=0.0.4,<=0.0.5"
# - openbmb/MiniCPM-V-2
timm = ">=1.0.13,<=1.0.15"
# - Qwen/Qwen-7B
# - Qwen/Qwen-7B-Chat
# - Salesforce/xgen-7b-8k-base
tiktoken = ">=0.8.0,<=0.9.0"
# - microsoft/biogpt
sacremoses = "==0.1.1"
# - openai/whisper-base
librosa = ">=0.10.1,<=0.11.0"
soundfile = ">=0.12.0,<=0.13.1"
datasets = ">=3.3.0,<=3.5.1"
rouge = "==1.0.1"

[tool.poetry.group.wwb]
optional = true

[tool.poetry.group.wwb.dependencies]
accelerate = ">=0.26.0,<=1.6.0"
transformers = ">=4.35.2,<=4.51.3"
sentence-transformers = ">=2.0.0,<=4.1.0"
pandas = ">=2.0.3,<=2.2.3"
numpy = ">=1.23.5,<=2.2.5"
tqdm = ">=4.66.1,<=4.67.1"
diffusers = ">=0.30.0,<=0.33.1"
datasets = ">=2.17.0,<=3.5.1"
auto-gptq = { version = ">=0.5.0,<=0.7.1", markers = "sys_platform == 'linux'" }
autoawq = { version = ">=0.2.0,<=0.2.7", markers = "sys_platform == 'linux'" }
sentencepiece = ">=0.1.98,<=0.2.0"
